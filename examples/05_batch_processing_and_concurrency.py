# examples/05_batch_processing_and_concurrency.py
"""
Trans-Hub v3.0 æ‰¹é‡å¤„ç†ä¸Žå¹¶å‘ç¤ºä¾‹

æœ¬ç¤ºä¾‹å±•ç¤ºäº†ç³»ç»Ÿå¤„ç†å¤§é‡ä»»åŠ¡çš„èƒ½åŠ›ï¼š
1. åœ¨ä¸€ä¸ªå¾ªçŽ¯ä¸­å¿«é€Ÿæäº¤å¤§é‡ï¼ˆä¾‹å¦‚100ä¸ªï¼‰ç‹¬ç«‹çš„ç¿»è¯‘è¯·æ±‚ã€‚
2. å¯åŠ¨å¤šä¸ªå¹¶å‘çš„ Worker (AsyncIO Task) æ¥åŒæ—¶å¤„ç†ä¸åŒè¯­è¨€çš„ä»»åŠ¡ã€‚
3. ç»Ÿè®¡å¹¶éªŒè¯æ‰€æœ‰ä»»åŠ¡æ˜¯å¦éƒ½å·²æˆåŠŸå¤„ç†ã€‚
"""
import asyncio
import os
import sys
import time
from pathlib import Path

import structlog

# --- è·¯å¾„è®¾ç½® ---
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))
# ---

from trans_hub import Coordinator, TransHubConfig  # noqa: E402
from trans_hub.core import TranslationResult  # noqa: E402
from trans_hub.logging_config import setup_logging  # noqa: E402
from trans_hub.persistence import create_persistence_handler  # noqa: E402

# --- æ—¥å¿—é…ç½® ---
setup_logging(log_level="WARNING")  # è®¾ç½®ä¸º WARNING ä»¥é¿å…å¤§é‡INFOæ—¥å¿—åˆ·å±
log = structlog.get_logger(__name__)

# --- å‡†å¤‡æµ‹è¯•çŽ¯å¢ƒ ---
DB_FILE = "th_example_05.db"
NUM_TASKS = 100
TARGET_LANGS = ["de", "fr", "es"]


async def main() -> None:
    """æ‰§è¡Œæ‰¹é‡å¤„ç†ä¸Žå¹¶å‘ç¤ºä¾‹ã€‚"""
    if os.path.exists(DB_FILE):
        os.remove(DB_FILE)

    config = TransHubConfig(database_url=f"sqlite:///{DB_FILE}", source_lang="en")
    handler = create_persistence_handler(config)
    coordinator = Coordinator(config=config, persistence_handler=handler)

    try:
        await coordinator.initialize()

        # 1. å¿«é€Ÿæäº¤å¤§é‡ä»»åŠ¡
        log.warning(f"ðŸš€ æ­¥éª¤ 1: æ­£åœ¨å¿«é€Ÿæäº¤ {NUM_TASKS} ä¸ªç¿»è¯‘è¯·æ±‚...")
        start_time = time.monotonic()
        request_tasks = []
        for i in range(NUM_TASKS):
            task = coordinator.request(
                business_id=f"item.{i}",
                source_payload={"text": f"This is item number {i}"},
                target_langs=TARGET_LANGS,
            )
            request_tasks.append(task)
        await asyncio.gather(*request_tasks)
        duration = time.monotonic() - start_time
        log.warning(
            f"âœ… {NUM_TASKS * len(TARGET_LANGS)} ä¸ªä»»åŠ¡æ¡ç›®æäº¤å®Œæ¯•ï¼Œè€—æ—¶: {duration:.2f}s"
        )

        # 2. å¯åŠ¨å¤šä¸ªå¹¶å‘ Worker
        log.warning(f"ðŸ‘· æ­¥éª¤ 2: å¯åŠ¨ {len(TARGET_LANGS)} ä¸ªå¹¶å‘ Worker...")
        start_time = time.monotonic()
        worker_tasks = [
            asyncio.create_task(consume_all(coordinator, lang))
            for lang in TARGET_LANGS
        ]
        
        # ç­‰å¾…æ‰€æœ‰ Worker å®Œæˆ
        results_per_lang = await asyncio.gather(*worker_tasks)
        duration = time.monotonic() - start_time

        # 3. éªŒè¯ç»“æžœ
        log.warning("ðŸ” æ­¥éª¤ 3: éªŒè¯å¤„ç†ç»“æžœ...")
        total_processed = sum(len(results) for results in results_per_lang)
        log.warning(
            f"ðŸŽ‰ æ‰€æœ‰ Worker å¤„ç†å®Œæ¯•ï¼Œå…±å¤„ç† {total_processed} ä¸ªä»»åŠ¡ï¼Œè€—æ—¶: {duration:.2f}s"
        )
        assert total_processed == NUM_TASKS * len(TARGET_LANGS)
        log.warning("âœ… éªŒè¯é€šè¿‡ï¼æ‰€æœ‰ä»»åŠ¡å‡å·²æˆåŠŸå¤„ç†ã€‚")

    finally:
        await coordinator.close()
        if os.path.exists(DB_FILE):
            os.remove(DB_FILE)


async def consume_all(coordinator: Coordinator, lang: str) -> list[TranslationResult]:
    """æ¶ˆè´¹æŒ‡å®šè¯­è¨€çš„æ‰€æœ‰å¾…åŠžä»»åŠ¡å¹¶è¿”å›žç»“æžœåˆ—è¡¨ã€‚"""
    return [res async for res in coordinator.process_pending_translations(lang)]


if __name__ == "__main__":
    asyncio.run(main())