# examples/05_batch_processing_and_concurrency.py
"""
Trans-Hub v3.0 æ‰¹é‡å¤„ç†ä¸å¹¶å‘ç¤ºä¾‹

æœ¬ç¤ºä¾‹å±•ç¤ºäº†ç³»ç»Ÿå¤„ç†å¤§é‡ä»»åŠ¡çš„èƒ½åŠ›ï¼š
1. åœ¨ä¸€ä¸ªå¾ªç¯ä¸­å¿«é€Ÿæäº¤å¤§é‡ï¼ˆä¾‹å¦‚100ä¸ªï¼‰ç‹¬ç«‹çš„ç¿»è¯‘è¯·æ±‚ã€‚
2. å¯åŠ¨å¤šä¸ªå¹¶å‘çš„ Worker (AsyncIO Task) æ¥åŒæ—¶å¤„ç†ä¸åŒè¯­è¨€çš„ä»»åŠ¡ã€‚
3. ç»Ÿè®¡å¹¶éªŒè¯æ‰€æœ‰ä»»åŠ¡æ˜¯å¦éƒ½å·²æˆåŠŸå¤„ç†ã€‚

è¿è¡Œæ–¹å¼:
åœ¨é¡¹ç›®æ ¹ç›®å½•æ‰§è¡Œ: `poetry run python examples/05_batch_processing_and_concurrency.py`
"""

import asyncio
import sys
import time
from pathlib import Path

import structlog

# --- è·¯å¾„è®¾ç½® ---
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root))
# ---

from trans_hub import Coordinator, TransHubConfig  # noqa: E402
from trans_hub.core import TranslationResult  # noqa: E402
from trans_hub.db.schema_manager import apply_migrations  # noqa: E402
from trans_hub.logging_config import setup_logging  # noqa: E402
from trans_hub.persistence import create_persistence_handler  # noqa: E402

# --- æ—¥å¿—é…ç½® ---
setup_logging(log_level="WARNING")
log = structlog.get_logger("trans_hub")

# --- å‡†å¤‡æµ‹è¯•ç¯å¢ƒ ---
DB_FILE = Path(__file__).parent / "th_example_05.db"
NUM_TASKS = 100
TARGET_LANGS = ["de", "fr", "es"]


async def main() -> None:
    """æ‰§è¡Œæ‰¹é‡å¤„ç†ä¸å¹¶å‘ç¤ºä¾‹ã€‚"""
    if DB_FILE.exists():
        DB_FILE.unlink()

    config = TransHubConfig(
        database_url=f"sqlite:///{DB_FILE.resolve()}", source_lang="en"
    )
    apply_migrations(config.db_path)
    handler = create_persistence_handler(config)
    coordinator = Coordinator(config=config, persistence_handler=handler)

    try:
        await coordinator.initialize()
        log.warning("âœ… åè°ƒå™¨åˆå§‹åŒ–æˆåŠŸ", db_path=str(DB_FILE))

        log.warning(f"ğŸš€ æ­¥éª¤ 1: æ­£åœ¨å¿«é€Ÿæäº¤ {NUM_TASKS} ä¸ªç¿»è¯‘è¯·æ±‚...")
        start_time = time.monotonic()
        request_tasks = []
        for i in range(NUM_TASKS):
            task = coordinator.request(
                business_id=f"item.{i}",
                source_payload={"text": f"This is item number {i}"},
                target_langs=TARGET_LANGS,
            )
            request_tasks.append(task)
        await asyncio.gather(*request_tasks)
        duration = time.monotonic() - start_time
        log.warning(
            f"âœ… {NUM_TASKS * len(TARGET_LANGS)} ä¸ªä»»åŠ¡æ¡ç›®æäº¤å®Œæ¯•ï¼Œè€—æ—¶: {duration:.2f}s"
        )

        log.warning(f"ğŸ‘· æ­¥éª¤ 2: å¯åŠ¨ {len(TARGET_LANGS)} ä¸ªå¹¶å‘ Worker...")
        start_time = time.monotonic()
        results_per_lang = await process_translations_with_results(
            coordinator, TARGET_LANGS
        )
        duration = time.monotonic() - start_time

        log.warning("ğŸ” æ­¥éª¤ 3: éªŒè¯å¤„ç†ç»“æœ...")
        total_processed = sum(len(results) for results in results_per_lang)
        log.warning(
            f"ğŸ‰ æ‰€æœ‰ Worker å¤„ç†å®Œæ¯•ï¼Œå…±å¤„ç† {total_processed} ä¸ªä»»åŠ¡ï¼Œè€—æ—¶: {duration:.2f}s"
        )
        assert total_processed == NUM_TASKS * len(TARGET_LANGS)
        log.warning("âœ… éªŒè¯é€šè¿‡ï¼æ‰€æœ‰ä»»åŠ¡å‡å·²æˆåŠŸå¤„ç†ã€‚")

    finally:
        await coordinator.close()
        log.warning("ğŸšª åè°ƒå™¨å·²å…³é—­")
        if DB_FILE.exists():
            DB_FILE.unlink()


async def process_translations_with_results(
    coordinator: Coordinator, langs: list[str]
) -> list[list[TranslationResult]]:
    """æ¨¡æ‹Ÿ Worker å¤„ç†æ‰€æœ‰å¾…åŠä»»åŠ¡å¹¶è¿”å›ç»“æœã€‚"""
    tasks = [
        asyncio.create_task(consume_all_and_return(coordinator, lang)) for lang in langs
    ]
    return await asyncio.gather(*tasks)


async def consume_all_and_return(
    coordinator: Coordinator, lang: str
) -> list[TranslationResult]:
    """æ¶ˆè´¹æŒ‡å®šè¯­è¨€çš„æ‰€æœ‰å¾…åŠä»»åŠ¡å¹¶è¿”å›ç»“æœåˆ—è¡¨ã€‚"""
    results = [res async for res in coordinator.process_pending_translations(lang)]
    log.info(f"Worker ä¸ºè¯­è¨€ '{lang}' å¤„ç†äº† {len(results)} ä¸ªä»»åŠ¡ã€‚")
    return results


if __name__ == "__main__":
    asyncio.run(main())
