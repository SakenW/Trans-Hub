# run_coordinator_test.py (v1.1 æœ€ç»ˆç‰ˆ)
import os
import shutil
import time
from typing import Any, Dict, List

import structlog

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
from dotenv import load_dotenv

# æœ¬åœ°åº“å¯¼å…¥
from trans_hub.config import EngineConfigs, TransHubConfig
from trans_hub.coordinator import Coordinator
from trans_hub.db.schema_manager import apply_migrations
from trans_hub.engines.debug import DebugEngineConfig
from trans_hub.logging_config import setup_logging
from trans_hub.persistence import DefaultPersistenceHandler
from trans_hub.rate_limiter import RateLimiter
from trans_hub.types import TranslationStatus

# è·å–ä¸€ä¸ª logger
log = structlog.get_logger()

# å®šä¹‰ä¸€ä¸ªä¸´æ—¶çš„æµ‹è¯•ç›®å½•å’Œæ•°æ®åº“æ–‡ä»¶
TEST_DIR = "temp_test_data"
DB_FILE = os.path.join(TEST_DIR, "test_transhub.db")


def setup_test_environment():
    """åˆ›å»ºä¸€ä¸ªå¹²å‡€çš„æµ‹è¯•ç¯å¢ƒã€‚"""
    log.info("--- æ­£åœ¨è®¾ç½®æµ‹è¯•ç¯å¢ƒ ---")
    if os.path.exists(TEST_DIR):
        shutil.rmtree(TEST_DIR)
    os.makedirs(TEST_DIR)
    apply_migrations(DB_FILE)
    log.info("æµ‹è¯•ç¯å¢ƒå·²å°±ç»ªã€‚")


def cleanup_test_environment():
    """æ¸…ç†æµ‹è¯•ç¯å¢ƒã€‚"""
    log.info("--- æ­£åœ¨æ¸…ç†æµ‹è¯•ç¯å¢ƒ ---")
    if os.path.exists(TEST_DIR):
        shutil.rmtree(TEST_DIR)
    log.info("æµ‹è¯•ç¯å¢ƒå·²æ¸…ç†ã€‚")


def test_full_workflow():
    """
    æµ‹è¯• Trans-Hub çš„å®Œæ•´ç«¯åˆ°ç«¯å·¥ä½œæµï¼ŒåŒ…æ‹¬ï¼š
    1. é¦–æ¬¡ç¿»è¯‘ä¸ç¼“å­˜ã€‚
    2. ä¸Šä¸‹æ–‡ç¿»è¯‘ã€‚
    3. é”™è¯¯å¤„ç†ä¸é‡è¯•ã€‚
    4. é€Ÿç‡é™åˆ¶ã€‚
    5. åƒåœ¾å›æ”¶ (GC)ã€‚
    """
    log.info("====== å¼€å§‹ Trans-Hub v1.1 å®Œæ•´å·¥ä½œæµæµ‹è¯• ======")

    # --- 1. åˆå§‹åŒ– Coordinator ---
    handler = DefaultPersistenceHandler(db_path=DB_FILE)
    # ä½¿ç”¨ DebugEngine è¿›è¡Œå¯é¢„æµ‹çš„æµ‹è¯•
    debug_config = DebugEngineConfig(
        mode="SUCCESS",
        translation_map={
            "Hello, world!": "ä½ å¥½ï¼Œè°ƒè¯•ä¸–ç•Œï¼",
            "Apple": "è‹¹æœ (æ°´æœä¸Šä¸‹æ–‡)",
            "Bank": "é“¶è¡Œ (é‡‘èä¸Šä¸‹æ–‡)",
        },
        # é…ç½®ä¸€ä¸ªå¯é‡è¯•çš„å¤±è´¥åœºæ™¯
        fail_on_text="retry_me",
        fail_is_retryable=True,
    )
    config = TransHubConfig(
        database_url=f"sqlite:///{DB_FILE}",
        active_engine="debug",
        engine_configs=EngineConfigs(debug=debug_config),
        gc_retention_days=0,  # è®¾ç½®ä¸º0å¤©ï¼Œæ–¹ä¾¿æµ‹è¯•GC
    )
    # é…ç½®ä¸€ä¸ªé€Ÿç‡é™åˆ¶å™¨ï¼Œæ¯ç§’2ä¸ªè¯·æ±‚
    rate_limiter = RateLimiter(rate=2, capacity=2)
    coordinator = Coordinator(
        config=config, persistence_handler=handler, rate_limiter=rate_limiter
    )

    try:
        # --- 2. æµ‹è¯•é¦–æ¬¡ç¿»è¯‘ä¸ä¸Šä¸‹æ–‡ ---
        log.info("\n--- æµ‹è¯•é˜¶æ®µï¼šé¦–æ¬¡ç¿»è¯‘ä¸ä¸Šä¸‹æ–‡ ---")
        tasks_to_request: List[Dict[str, Any]] = [
            {"text": "Hello, world!", "context": None, "business_id": "greeting.hello"},
            {
                "text": "Apple",
                "context": {"category": "fruit"},
                "business_id": "food.apple",
            },
            {
                "text": "Bank",
                "context": {"type": "financial_institution"},
                "business_id": "finance.bank",
            },
            {
                "text": "This item will be garbage collected",
                "context": None,
                "business_id": "legacy.item",
            },
        ]
        for task in tasks_to_request:
            coordinator.request(
                target_langs=["jp"],
                text_content=task["text"],
                context=task["context"],
                business_id=task["business_id"],
            )

        results = list(coordinator.process_pending_translations(target_lang="jp"))
        assert len(results) == 4
        log.info("âœ… é¦–æ¬¡ç¿»è¯‘ä¸ä¸Šä¸‹æ–‡æµ‹è¯•æˆåŠŸï¼")

        # --- 3. æµ‹è¯•ç¼“å­˜ ---
        log.info("\n--- æµ‹è¯•é˜¶æ®µï¼šç¼“å­˜å‘½ä¸­ ---")
        # å†æ¬¡è¯·æ±‚ï¼Œè¿™æ¬¡ä¸åº”è¯¥æœ‰ä»»ä½•æ–°ä»»åŠ¡è¢«å¤„ç†
        coordinator.request(
            target_langs=["jp"],
            text_content="Hello, world!",
            business_id="greeting.hello",
        )
        cached_run_results = list(
            coordinator.process_pending_translations(target_lang="jp")
        )
        assert (
            len(cached_run_results) == 0
        ), "ç¼“å­˜å‘½ä¸­æ—¶ï¼Œprocess_pending_translations ä¸åº”è¿”å›ä»»ä½•ç»“æœ"
        # ç›´æ¥æŸ¥è¯¢ç¼“å­˜è¿›è¡ŒéªŒè¯
        cached_result = coordinator.handler.get_translation("Hello, world!", "jp")
        assert cached_result is not None and cached_result.from_cache is True
        log.info("âœ… ç¼“å­˜å‘½ä¸­æµ‹è¯•æˆåŠŸï¼")

        # --- 4. æµ‹è¯•é”™è¯¯å¤„ç†ä¸é‡è¯• ---
        log.info("\n--- æµ‹è¯•é˜¶æ®µï¼šé”™è¯¯å¤„ç†ä¸é‡è¯• ---")
        coordinator.request(
            target_langs=["jp"], text_content="retry_me", business_id="test.retry"
        )
        # é…ç½® DebugEngine åœ¨ä¸‹ä¸€æ¬¡è°ƒç”¨æ—¶æˆåŠŸ
        coordinator.active_engine.config.fail_on_text = None

        retry_results = list(
            coordinator.process_pending_translations(
                target_lang="jp", max_retries=1, initial_backoff=0.1
            )
        )
        assert (
            len(retry_results) == 1
            and retry_results[0].status == TranslationStatus.TRANSLATED
        )
        log.info("âœ… é”™è¯¯å¤„ç†ä¸é‡è¯•æµ‹è¯•æˆåŠŸï¼")

        # --- 5. æµ‹è¯•é€Ÿç‡é™åˆ¶ ---
        log.info("\n--- æµ‹è¯•é˜¶æ®µï¼šé€Ÿç‡é™åˆ¶ ---")
        # åˆ›å»º3ä¸ªæ–°ä»»åŠ¡ï¼Œbatch_size=1, é€Ÿç‡ä¸º2/sï¼Œè‡³å°‘éœ€è¦0.5ç§’
        rate_limit_tasks = [
            {"text": "rate_1", "business_id": "rate.1"},
            {"text": "rate_2", "business_id": "rate.2"},
            {"text": "rate_3", "business_id": "rate.3"},
        ]
        for task in rate_limit_tasks:
            coordinator.request(
                target_langs=["jp"],
                text_content=task["text"],
                business_id=task["business_id"],
            )

        start_time = time.monotonic()
        rate_limit_results = list(
            coordinator.process_pending_translations(target_lang="jp", batch_size=1)
        )
        duration = time.monotonic() - start_time
        assert len(rate_limit_results) == 3
        assert duration > 0.45, f"é€Ÿç‡é™åˆ¶å™¨æœªç”Ÿæ•ˆ (duration: {duration:.2f}s)"
        log.info(f"âœ… é€Ÿç‡é™åˆ¶æµ‹è¯•æˆåŠŸï¼(è€—æ—¶: {duration:.2f}s)")

        # --- 6. æµ‹è¯•åƒåœ¾å›æ”¶ ---
        log.info("\n--- æµ‹è¯•é˜¶æ®µï¼šåƒåœ¾å›æ”¶ (GC) ---")
        # 'legacy.item' business_id åœ¨æ­¤ä¹‹åæ²¡æœ‰è¢«å†æ¬¡ request
        # æˆ‘ä»¬åªæ›´æ–°äº† 'greeting.hello', 'test.retry', å’Œ rate.* çš„ last_seen_at
        gc_stats_dry = coordinator.run_garbage_collection(dry_run=True)
        # é¢„ä¼°å°†æ¸…ç† food.apple å’Œ finance.bank ä»¥åŠ legacy.item
        # å› ä¸ºå®ƒä»¬åœ¨ç¬¬3æ­¥ä¹‹åæ²¡æœ‰è¢«å†æ¬¡ request
        assert gc_stats_dry["deleted_sources"] == 3, "GC dry_run åº”è¯¥é¢„ä¼°æ¸…ç†3ä¸ªè¿‡æ—¶çš„æº"

        gc_stats_real = coordinator.run_garbage_collection(dry_run=False)
        assert gc_stats_real["deleted_sources"] == 3, "GC åº”è¯¥å®é™…æ¸…ç†äº†3ä¸ªè¿‡æ—¶çš„æº"

        with handler.transaction() as cursor:
            cursor.execute("SELECT COUNT(*) FROM th_sources")
            # å‰©ä¸‹ 'greeting.hello', 'test.retry', å’Œ3ä¸ªrate.* çš„æºï¼Œå…±5ä¸ª
            remaining_sources = cursor.fetchone()[0]
            assert remaining_sources == 5, f"GCååº”å‰©ä¸‹5ä¸ªæºï¼Œå®é™…ä¸º{remaining_sources}"
        log.info("âœ… åƒåœ¾å›æ”¶ (GC) æµ‹è¯•æˆåŠŸï¼")

    finally:
        if coordinator:
            coordinator.close()


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•ã€‚"""
    load_dotenv()
    setup_logging(log_level="INFO")

    root_log = structlog.get_logger("test_runner")
    root_log.info("======== Trans-Hub v1.1 åŠŸèƒ½éªŒè¯å¼€å§‹ ========")

    try:
        setup_test_environment()
        test_full_workflow()
        root_log.info("ğŸ‰======== æ‰€æœ‰æµ‹è¯•æˆåŠŸé€šè¿‡ï¼Trans-Hub v1.1 åŠŸèƒ½éªŒè¯å®Œæˆï¼========ğŸ‰")
    except Exception:
        root_log.error("âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸ï¼", exc_info=True)
        raise
    finally:
        cleanup_test_environment()


if __name__ == "__main__":
    main()
