# packages/server/examples/05_quality_assurance.py
"""
ç¤ºä¾‹ 5ï¼šç¿»è¯‘è´¨é‡ä¿è¯

æœ¬ç¤ºä¾‹å±•ç¤ºäº†ç¿»è¯‘è´¨é‡æ§åˆ¶çš„å®Œæ•´ä½“ç³»ï¼š
1. è‡ªåŠ¨åŒ–è´¨é‡æ£€æŸ¥
2. æœ¯è¯­ä¸€è‡´æ€§éªŒè¯
3. æ ¼å¼å’Œæ ·å¼æ£€æŸ¥
4. è´¨é‡è¯„åˆ†ç³»ç»Ÿ
5. è´¨é‡æŠ¥å‘Šå’Œæ”¹è¿›å»ºè®®

é€‚ç”¨äºå¯¹ç¿»è¯‘è´¨é‡æœ‰ä¸¥æ ¼è¦æ±‚çš„ä¼ä¸šçº§åœºæ™¯ã€‚
"""

import asyncio
import re
from typing import Dict, List, Optional
from dataclasses import dataclass
from enum import Enum
import structlog
from _shared import example_runner, print_section_header, print_step, print_success

logger = structlog.get_logger()


class QualityIssueType(Enum):
    """è´¨é‡é—®é¢˜ç±»å‹ã€‚"""
    TERMINOLOGY = "terminology"          # æœ¯è¯­ä¸ä¸€è‡´
    FORMATTING = "formatting"            # æ ¼å¼é—®é¢˜
    LENGTH = "length"                    # é•¿åº¦é—®é¢˜
    COMPLETENESS = "completeness"        # å®Œæ•´æ€§é—®é¢˜
    STYLE = "style"                      # é£æ ¼é—®é¢˜
    GRAMMAR = "grammar"                  # è¯­æ³•é—®é¢˜


class QualitySeverity(Enum):
    """è´¨é‡é—®é¢˜ä¸¥é‡ç¨‹åº¦ã€‚"""
    CRITICAL = "critical"    # ä¸¥é‡ï¼šå¿…é¡»ä¿®å¤
    MAJOR = "major"          # é‡è¦ï¼šå»ºè®®ä¿®å¤
    MINOR = "minor"          # è½»å¾®ï¼šå¯é€‰ä¿®å¤
    INFO = "info"            # ä¿¡æ¯ï¼šä»…æç¤º


@dataclass
class QualityIssue:
    """è´¨é‡é—®é¢˜æ•°æ®ç»“æ„ã€‚"""
    type: QualityIssueType
    severity: QualitySeverity
    message: str
    position: Optional[int] = None
    suggestion: Optional[str] = None


@dataclass
class QualityReport:
    """è´¨é‡æŠ¥å‘Šæ•°æ®ç»“æ„ã€‚"""
    content_id: str
    target_lang: str
    overall_score: float
    issues: List[QualityIssue]
    passed_checks: List[str]
    failed_checks: List[str]


# æœ¯è¯­è¯å…¸
TERMINOLOGY_DICT = {
    "zh-CN": {
        "user": "ç”¨æˆ·",
        "login": "ç™»å½•",
        "password": "å¯†ç ",
        "database": "æ•°æ®åº“",
        "authentication": "èº«ä»½éªŒè¯",
        "authorization": "æˆæƒ",
        "configuration": "é…ç½®",
        "settings": "è®¾ç½®",
        "profile": "ä¸ªäººèµ„æ–™",
        "dashboard": "ä»ªè¡¨æ¿"
    },
    "ja-JP": {
        "user": "ãƒ¦ãƒ¼ã‚¶ãƒ¼",
        "login": "ãƒ­ã‚°ã‚¤ãƒ³",
        "password": "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰",
        "database": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹",
        "authentication": "èªè¨¼",
        "authorization": "èªå¯",
        "configuration": "è¨­å®š",
        "settings": "è¨­å®š",
        "profile": "ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«",
        "dashboard": "ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰"
    }
}

# æµ‹è¯•ç¿»è¯‘æ ·æœ¬
TRANSLATION_SAMPLES = [
    {
        "content_id": "sample_001",
        "source_text": "Please enter your user login credentials to access the dashboard.",
        "target_lang": "zh-CN",
        "target_text": "è¯·è¾“å…¥æ‚¨çš„ç”¨æˆ·ç™»é™†å‡­æ®ä»¥è®¿é—®ä»ªè¡¨ç›˜ã€‚",  # æ•…æ„åŒ…å«é”™è¯¯
        "namespace": "ui.auth"
    },
    {
        "content_id": "sample_002",
        "source_text": "Database configuration settings",
        "target_lang": "zh-CN",
        "target_text": "æ•°æ®åº“é…ç½®è®¾ç½®",
        "namespace": "docs.config"
    },
    {
        "content_id": "sample_003",
        "source_text": "User authentication failed. Please check your password.",
        "target_lang": "zh-CN",
        "target_text": "ç”¨æˆ·è®¤è¯å¤±è´¥ã€‚è¯·æ£€æŸ¥æ‚¨çš„å¯†ç ã€‚",  # æœ¯è¯­ä¸ä¸€è‡´
        "namespace": "messages.error"
    },
    {
        "content_id": "sample_004",
        "source_text": "Login",
        "target_lang": "ja-JP",
        "target_text": "ãƒ­ã‚°ã‚¤ãƒ³",
        "namespace": "ui.buttons"
    },
    {
        "content_id": "sample_005",
        "source_text": "This is a very long sentence that contains multiple clauses and should be checked for appropriate length in the target language to ensure readability.",
        "target_lang": "zh-CN",
        "target_text": "è¿™æ˜¯ä¸€ä¸ªéå¸¸é•¿çš„å¥å­ï¼ŒåŒ…å«å¤šä¸ªä»å¥ï¼Œåº”è¯¥æ£€æŸ¥ç›®æ ‡è¯­è¨€ä¸­çš„é€‚å½“é•¿åº¦ä»¥ç¡®ä¿å¯è¯»æ€§ã€‚",
        "namespace": "docs.content"
    }
]


class QualityChecker:
    """ç¿»è¯‘è´¨é‡æ£€æŸ¥å™¨ã€‚"""
    
    def __init__(self):
        self.terminology_dict = TERMINOLOGY_DICT
    
    async def check_terminology_consistency(self, source_text: str, target_text: str, target_lang: str) -> List[QualityIssue]:
        """æ£€æŸ¥æœ¯è¯­ä¸€è‡´æ€§ã€‚"""
        issues = []
        
        if target_lang not in self.terminology_dict:
            return issues
        
        terms = self.terminology_dict[target_lang]
        source_lower = source_text.lower()
        
        for english_term, correct_translation in terms.items():
            if english_term in source_lower:
                # æ£€æŸ¥ç›®æ ‡æ–‡æœ¬æ˜¯å¦ä½¿ç”¨äº†æ­£ç¡®çš„æœ¯è¯­
                if correct_translation not in target_text:
                    # æŸ¥æ‰¾å¯èƒ½çš„é”™è¯¯æœ¯è¯­
                    wrong_terms = self._find_similar_terms(target_text, correct_translation)
                    if wrong_terms:
                        issue = QualityIssue(
                            type=QualityIssueType.TERMINOLOGY,
                            severity=QualitySeverity.MAJOR,
                            message=f"æœ¯è¯­ä¸ä¸€è‡´ï¼š'{english_term}' åº”ç¿»è¯‘ä¸º '{correct_translation}'ï¼Œä½†å‘ç°äº† '{wrong_terms[0]}'",
                            suggestion=f"å°† '{wrong_terms[0]}' æ›¿æ¢ä¸º '{correct_translation}'"
                        )
                        issues.append(issue)
        
        return issues
    
    def _find_similar_terms(self, text: str, correct_term: str) -> List[str]:
        """æŸ¥æ‰¾ç›¸ä¼¼çš„é”™è¯¯æœ¯è¯­ã€‚"""
        # ç®€åŒ–çš„ç›¸ä¼¼æœ¯è¯­æ£€æµ‹
        similar_terms_map = {
            "ç™»å½•": ["ç™»é™†"],
            "èº«ä»½éªŒè¯": ["è®¤è¯", "éªŒè¯"],
            "ä»ªè¡¨æ¿": ["ä»ªè¡¨ç›˜", "æ§åˆ¶å°"]
        }
        
        if correct_term in similar_terms_map:
            for wrong_term in similar_terms_map[correct_term]:
                if wrong_term in text:
                    return [wrong_term]
        
        return []
    
    async def check_formatting(self, source_text: str, target_text: str) -> List[QualityIssue]:
        """æ£€æŸ¥æ ¼å¼é—®é¢˜ã€‚"""
        issues = []
        
        # æ£€æŸ¥æ ‡ç‚¹ç¬¦å·
        source_punctuation = re.findall(r'[.!?]', source_text)
        target_punctuation = re.findall(r'[ã€‚ï¼ï¼Ÿ.!?]', target_text)
        
        if len(source_punctuation) != len(target_punctuation):
            issue = QualityIssue(
                type=QualityIssueType.FORMATTING,
                severity=QualitySeverity.MINOR,
                message=f"æ ‡ç‚¹ç¬¦å·æ•°é‡ä¸åŒ¹é…ï¼šæºæ–‡æœ¬ {len(source_punctuation)} ä¸ªï¼Œç›®æ ‡æ–‡æœ¬ {len(target_punctuation)} ä¸ª",
                suggestion="æ£€æŸ¥å¹¶è°ƒæ•´æ ‡ç‚¹ç¬¦å·"
            )
            issues.append(issue)
        
        # æ£€æŸ¥ç©ºæ ¼
        if target_text.startswith(' ') or target_text.endswith(' '):
            issue = QualityIssue(
                type=QualityIssueType.FORMATTING,
                severity=QualitySeverity.MINOR,
                message="æ–‡æœ¬å¼€å¤´æˆ–ç»“å°¾åŒ…å«å¤šä½™ç©ºæ ¼",
                suggestion="åˆ é™¤å¤šä½™çš„ç©ºæ ¼"
            )
            issues.append(issue)
        
        return issues
    
    async def check_length_appropriateness(self, source_text: str, target_text: str, namespace: str) -> List[QualityIssue]:
        """æ£€æŸ¥é•¿åº¦é€‚å½“æ€§ã€‚"""
        issues = []
        
        source_len = len(source_text)
        target_len = len(target_text)
        
        # æ ¹æ®å‘½åç©ºé—´è®¾ç½®ä¸åŒçš„é•¿åº¦é™åˆ¶
        if namespace.startswith("ui."):
            # UIå…ƒç´ åº”è¯¥ç®€æ´
            if target_len > source_len * 1.5:
                issue = QualityIssue(
                    type=QualityIssueType.LENGTH,
                    severity=QualitySeverity.MAJOR,
                    message=f"UIæ–‡æœ¬è¿‡é•¿ï¼šç›®æ ‡æ–‡æœ¬ {target_len} å­—ç¬¦ï¼Œæºæ–‡æœ¬ {source_len} å­—ç¬¦",
                    suggestion="è€ƒè™‘ä½¿ç”¨æ›´ç®€æ´çš„è¡¨è¾¾"
                )
                issues.append(issue)
        elif namespace.startswith("docs."):
            # æ–‡æ¡£å¯ä»¥é€‚å½“æ‰©å±•
            if target_len > source_len * 2.0:
                issue = QualityIssue(
                    type=QualityIssueType.LENGTH,
                    severity=QualitySeverity.MINOR,
                    message=f"æ–‡æ¡£æ–‡æœ¬è¾ƒé•¿ï¼šç›®æ ‡æ–‡æœ¬ {target_len} å­—ç¬¦ï¼Œæºæ–‡æœ¬ {source_len} å­—ç¬¦",
                    suggestion="æ£€æŸ¥æ˜¯å¦æœ‰å†—ä½™è¡¨è¾¾"
                )
                issues.append(issue)
        
        return issues
    
    async def check_completeness(self, source_text: str, target_text: str) -> List[QualityIssue]:
        """æ£€æŸ¥ç¿»è¯‘å®Œæ•´æ€§ã€‚"""
        issues = []
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
        if not target_text.strip():
            issue = QualityIssue(
                type=QualityIssueType.COMPLETENESS,
                severity=QualitySeverity.CRITICAL,
                message="ç¿»è¯‘å†…å®¹ä¸ºç©º",
                suggestion="æä¾›å®Œæ•´çš„ç¿»è¯‘"
            )
            issues.append(issue)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœªç¿»è¯‘çš„è‹±æ–‡ï¼ˆç®€å•æ£€æµ‹ï¼‰
        english_words = re.findall(r'\b[a-zA-Z]{3,}\b', target_text)
        if english_words and len(english_words) > 2:
            issue = QualityIssue(
                type=QualityIssueType.COMPLETENESS,
                severity=QualitySeverity.MAJOR,
                message=f"å¯èƒ½åŒ…å«æœªç¿»è¯‘çš„è‹±æ–‡å•è¯ï¼š{', '.join(english_words[:3])}",
                suggestion="æ£€æŸ¥å¹¶ç¿»è¯‘æ‰€æœ‰è‹±æ–‡å†…å®¹"
            )
            issues.append(issue)
        
        return issues
    
    async def calculate_quality_score(self, issues: List[QualityIssue]) -> float:
        """è®¡ç®—è´¨é‡åˆ†æ•°ã€‚"""
        if not issues:
            return 100.0
        
        # æ ¹æ®é—®é¢˜ä¸¥é‡ç¨‹åº¦æ‰£åˆ†
        penalty_map = {
            QualitySeverity.CRITICAL: 30,
            QualitySeverity.MAJOR: 15,
            QualitySeverity.MINOR: 5,
            QualitySeverity.INFO: 1
        }
        
        total_penalty = sum(penalty_map[issue.severity] for issue in issues)
        score = max(0, 100 - total_penalty)
        
        return score
    
    async def generate_quality_report(self, sample: Dict) -> QualityReport:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Šã€‚"""
        all_issues = []
        passed_checks = []
        failed_checks = []
        
        # æ‰§è¡Œå„é¡¹æ£€æŸ¥
        terminology_issues = await self.check_terminology_consistency(
            sample["source_text"], sample["target_text"], sample["target_lang"]
        )
        all_issues.extend(terminology_issues)
        
        formatting_issues = await self.check_formatting(
            sample["source_text"], sample["target_text"]
        )
        all_issues.extend(formatting_issues)
        
        length_issues = await self.check_length_appropriateness(
            sample["source_text"], sample["target_text"], sample["namespace"]
        )
        all_issues.extend(length_issues)
        
        completeness_issues = await self.check_completeness(
            sample["source_text"], sample["target_text"]
        )
        all_issues.extend(completeness_issues)
        
        # ç»Ÿè®¡æ£€æŸ¥ç»“æœ
        check_results = {
            "æœ¯è¯­ä¸€è‡´æ€§": len(terminology_issues) == 0,
            "æ ¼å¼è§„èŒƒ": len(formatting_issues) == 0,
            "é•¿åº¦é€‚å½“": len(length_issues) == 0,
            "ç¿»è¯‘å®Œæ•´": len(completeness_issues) == 0
        }
        
        for check_name, passed in check_results.items():
            if passed:
                passed_checks.append(check_name)
            else:
                failed_checks.append(check_name)
        
        # è®¡ç®—æ€»åˆ†
        overall_score = await self.calculate_quality_score(all_issues)
        
        return QualityReport(
            content_id=sample["content_id"],
            target_lang=sample["target_lang"],
            overall_score=overall_score,
            issues=all_issues,
            passed_checks=passed_checks,
            failed_checks=failed_checks
        )


async def setup_quality_system(coordinator) -> QualityChecker:
    """
    è®¾ç½®è´¨é‡ä¿è¯ç³»ç»Ÿã€‚
    
    Args:
        coordinator: åè°ƒå™¨å®ä¾‹
    
    Returns:
        QualityChecker: è´¨é‡æ£€æŸ¥å™¨å®ä¾‹
    """
    print_step(1, "è®¾ç½®è´¨é‡ä¿è¯ç³»ç»Ÿ")
    
    checker = QualityChecker()
    
    # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šï¼š
    # - åŠ è½½æœ¯è¯­è¯å…¸
    # - é…ç½®è´¨é‡è§„åˆ™
    # - åˆå§‹åŒ–æ£€æŸ¥å¼•æ“
    
    print_success("è´¨é‡ä¿è¯ç³»ç»Ÿè®¾ç½®å®Œæˆ", 
                 terminology_entries=sum(len(terms) for terms in TERMINOLOGY_DICT.values()),
                 supported_languages=len(TERMINOLOGY_DICT))
    
    return checker


async def run_quality_checks(checker: QualityChecker) -> List[QualityReport]:
    """
    è¿è¡Œè´¨é‡æ£€æŸ¥ã€‚
    
    Args:
        checker: è´¨é‡æ£€æŸ¥å™¨
    
    Returns:
        List[QualityReport]: è´¨é‡æŠ¥å‘Šåˆ—è¡¨
    """
    print_step(2, f"å¯¹ {len(TRANSLATION_SAMPLES)} ä¸ªç¿»è¯‘æ ·æœ¬æ‰§è¡Œè´¨é‡æ£€æŸ¥")
    
    reports = []
    
    for sample in TRANSLATION_SAMPLES:
        print(f"   ğŸ” æ£€æŸ¥: {sample['content_id']} ({sample['target_lang']})")
        report = await checker.generate_quality_report(sample)
        reports.append(report)
        
        # æ˜¾ç¤ºæ£€æŸ¥ç»“æœ
        if report.issues:
            print(f"      âš ï¸  å‘ç° {len(report.issues)} ä¸ªé—®é¢˜ï¼Œè´¨é‡åˆ†æ•°: {report.overall_score:.1f}")
        else:
            print(f"      âœ… æ— é—®é¢˜ï¼Œè´¨é‡åˆ†æ•°: {report.overall_score:.1f}")
    
    print_success("è´¨é‡æ£€æŸ¥å®Œæˆ", samples_checked=len(TRANSLATION_SAMPLES))
    return reports


async def display_quality_reports(reports: List[QualityReport]) -> None:
    """
    æ˜¾ç¤ºè¯¦ç»†çš„è´¨é‡æŠ¥å‘Šã€‚
    
    Args:
        reports: è´¨é‡æŠ¥å‘Šåˆ—è¡¨
    """
    print_section_header("è¯¦ç»†è´¨é‡æŠ¥å‘Š", "ğŸ“‹")
    
    for i, report in enumerate(reports, 1):
        sample = next(s for s in TRANSLATION_SAMPLES if s["content_id"] == report.content_id)
        
        logger.info(
            "è´¨é‡æŠ¥å‘Šè¯¦æƒ…",
            æŠ¥å‘Šåºå·=i,
            å†…å®¹ID=report.content_id,
            æºæ–‡æœ¬=sample['source_text'],
            è¯‘æ–‡=sample['target_text'],
            è¯­è¨€=report.target_lang,
            è´¨é‡åˆ†æ•°=f"{report.overall_score:.1f}/100"
        )
        
        if report.passed_checks:
            logger.info("é€šè¿‡æ£€æŸ¥", æ£€æŸ¥é¡¹=', '.join(report.passed_checks))
        
        if report.failed_checks:
            logger.warning("æœªé€šè¿‡æ£€æŸ¥", æ£€æŸ¥é¡¹=', '.join(report.failed_checks))
        
        if report.issues:
            logger.info("å‘ç°çš„é—®é¢˜")
            for j, issue in enumerate(report.issues, 1):
                severity_icon = {
                    QualitySeverity.CRITICAL: "ğŸ”´",
                    QualitySeverity.MAJOR: "ğŸŸ¡",
                    QualitySeverity.MINOR: "ğŸŸ ",
                    QualitySeverity.INFO: "ğŸ”µ"
                }[issue.severity]
                
                logger.info(
                    "è´¨é‡é—®é¢˜",
                    åºå·=j,
                    ä¸¥é‡ç¨‹åº¦=severity_icon,
                    æ¶ˆæ¯=issue.message,
                    å»ºè®®=issue.suggestion if issue.suggestion else None
                )
        else:
            logger.info("ğŸ‰ æ— è´¨é‡é—®é¢˜")


async def generate_quality_statistics(reports: List[QualityReport]) -> None:
    """
    ç”Ÿæˆè´¨é‡ç»Ÿè®¡ä¿¡æ¯ã€‚
    
    Args:
        reports: è´¨é‡æŠ¥å‘Šåˆ—è¡¨
    """
    print_section_header("è´¨é‡ç»Ÿè®¡åˆ†æ", "ğŸ“Š")
    
    # æ€»ä½“ç»Ÿè®¡
    total_samples = len(reports)
    avg_score = sum(r.overall_score for r in reports) / total_samples
    high_quality = sum(1 for r in reports if r.overall_score >= 90)
    medium_quality = sum(1 for r in reports if 70 <= r.overall_score < 90)
    low_quality = sum(1 for r in reports if r.overall_score < 70)
    
    print("ğŸ“ˆ æ€»ä½“è´¨é‡æ¦‚è§ˆ:")
    print(f"   â€¢ æ ·æœ¬æ€»æ•°: {total_samples}")
    print(f"   â€¢ å¹³å‡åˆ†æ•°: {avg_score:.1f}/100")
    print(f"   â€¢ é«˜è´¨é‡ (â‰¥90åˆ†): {high_quality} ({high_quality/total_samples:.1%})")
    print(f"   â€¢ ä¸­ç­‰è´¨é‡ (70-89åˆ†): {medium_quality} ({medium_quality/total_samples:.1%})")
    print(f"   â€¢ ä½è´¨é‡ (<70åˆ†): {low_quality} ({low_quality/total_samples:.1%})")
    
    # é—®é¢˜ç±»å‹ç»Ÿè®¡
    issue_type_counts = {}
    issue_severity_counts = {}
    
    for report in reports:
        for issue in report.issues:
            issue_type_counts[issue.type] = issue_type_counts.get(issue.type, 0) + 1
            issue_severity_counts[issue.severity] = issue_severity_counts.get(issue.severity, 0) + 1
    
    if issue_type_counts:
        print("\nğŸ” é—®é¢˜ç±»å‹åˆ†å¸ƒ:")
        for issue_type, count in issue_type_counts.items():
            type_name = {
                QualityIssueType.TERMINOLOGY: "æœ¯è¯­é—®é¢˜",
                QualityIssueType.FORMATTING: "æ ¼å¼é—®é¢˜",
                QualityIssueType.LENGTH: "é•¿åº¦é—®é¢˜",
                QualityIssueType.COMPLETENESS: "å®Œæ•´æ€§é—®é¢˜",
                QualityIssueType.STYLE: "é£æ ¼é—®é¢˜",
                QualityIssueType.GRAMMAR: "è¯­æ³•é—®é¢˜"
            }[issue_type]
            print(f"   â€¢ {type_name}: {count} æ¬¡")
    
    if issue_severity_counts:
        print("\nâš ï¸  é—®é¢˜ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ:")
        for severity, count in issue_severity_counts.items():
            severity_name = {
                QualitySeverity.CRITICAL: "ä¸¥é‡",
                QualitySeverity.MAJOR: "é‡è¦",
                QualitySeverity.MINOR: "è½»å¾®",
                QualitySeverity.INFO: "ä¿¡æ¯"
            }[severity]
            print(f"   â€¢ {severity_name}: {count} æ¬¡")
    
    # è¯­è¨€è´¨é‡å¯¹æ¯”
    lang_scores = {}
    for report in reports:
        if report.target_lang not in lang_scores:
            lang_scores[report.target_lang] = []
        lang_scores[report.target_lang].append(report.overall_score)
    
    if len(lang_scores) > 1:
        print("\nğŸŒ è¯­è¨€è´¨é‡å¯¹æ¯”:")
        for lang, scores in lang_scores.items():
            avg_lang_score = sum(scores) / len(scores)
            print(f"   â€¢ {lang}: {avg_lang_score:.1f}/100 (æ ·æœ¬æ•°: {len(scores)})")


async def provide_improvement_suggestions(reports: List[QualityReport]) -> None:
    """
    æä¾›è´¨é‡æ”¹è¿›å»ºè®®ã€‚
    
    Args:
        reports: è´¨é‡æŠ¥å‘Šåˆ—è¡¨
    """
    print_section_header("è´¨é‡æ”¹è¿›å»ºè®®", "ğŸ’¡")
    
    # åˆ†æå¸¸è§é—®é¢˜
    common_issues = {}
    for report in reports:
        for issue in report.issues:
            key = (issue.type, issue.severity)
            if key not in common_issues:
                common_issues[key] = []
            common_issues[key].append(issue.message)
    
    # ç”Ÿæˆæ”¹è¿›å»ºè®®
    suggestions = []
    
    if QualityIssueType.TERMINOLOGY in [k[0] for k in common_issues.keys()]:
        suggestions.append("ğŸ¯ æœ¯è¯­ç®¡ç†: å»ºç«‹å¹¶ç»´æŠ¤ç»Ÿä¸€çš„æœ¯è¯­è¯å…¸ï¼Œç¡®ä¿è¯‘è€…ä½¿ç”¨ä¸€è‡´çš„æœ¯è¯­")
    
    if QualityIssueType.FORMATTING in [k[0] for k in common_issues.keys()]:
        suggestions.append("ğŸ“ æ ¼å¼è§„èŒƒ: åˆ¶å®šè¯¦ç»†çš„æ ¼å¼æŒ‡å—ï¼ŒåŒ…æ‹¬æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼ä½¿ç”¨è§„åˆ™")
    
    if QualityIssueType.LENGTH in [k[0] for k in common_issues.keys()]:
        suggestions.append("ğŸ“ é•¿åº¦æ§åˆ¶: ä¸ºä¸åŒç±»å‹çš„å†…å®¹è®¾ç½®åˆé€‚çš„é•¿åº¦é™åˆ¶å’ŒæŒ‡å¯¼åŸåˆ™")
    
    # é€šç”¨å»ºè®®
    suggestions.extend([
        "ğŸ”„ å®šæœŸåŸ¹è®­: ä¸ºè¯‘è€…æä¾›è´¨é‡æ ‡å‡†å’Œæœ€ä½³å®è·µåŸ¹è®­",
        "ğŸ¤– è‡ªåŠ¨åŒ–æ£€æŸ¥: é›†æˆæ›´å¤šè‡ªåŠ¨åŒ–è´¨é‡æ£€æŸ¥å·¥å…·",
        "ğŸ‘¥ åŒè¡Œè¯„è®®: å»ºç«‹è¯‘è€…é—´çš„ç›¸äº’è¯„è®®æœºåˆ¶",
        "ğŸ“Š æŒç»­ç›‘æ§: å®šæœŸåˆ†æè´¨é‡è¶‹åŠ¿ï¼ŒåŠæ—¶è°ƒæ•´ç­–ç•¥"
    ])
    
    print("ğŸš€ æ¨èçš„æ”¹è¿›æªæ–½:")
    for i, suggestion in enumerate(suggestions, 1):
        print(f"   {i}. {suggestion}")
    
    # è´¨é‡ç›®æ ‡
    print("\nğŸ¯ è´¨é‡ç›®æ ‡å»ºè®®:")
    print("   â€¢ çŸ­æœŸç›®æ ‡: å¹³å‡è´¨é‡åˆ†æ•°è¾¾åˆ° 85åˆ†")
    print("   â€¢ ä¸­æœŸç›®æ ‡: 90% çš„ç¿»è¯‘è¾¾åˆ°é«˜è´¨é‡æ ‡å‡† (â‰¥90åˆ†)")
    print("   â€¢ é•¿æœŸç›®æ ‡: å»ºç«‹é›¶ç¼ºé™·çš„ç¿»è¯‘è´¨é‡ä½“ç³»")


async def main() -> None:
    """æ‰§è¡Œç¿»è¯‘è´¨é‡ä¿è¯ç¤ºä¾‹ã€‚"""
    print_section_header("ç¿»è¯‘è´¨é‡ä¿è¯æ¼”ç¤º", "ğŸ›¡ï¸")
    
    async with example_runner("quality_assurance.db") as coordinator:
        # è®¾ç½®è´¨é‡ä¿è¯ç³»ç»Ÿ
        checker = await setup_quality_system(coordinator)
        
        # è¿è¡Œè´¨é‡æ£€æŸ¥
        reports = await run_quality_checks(checker)
        
        # æ˜¾ç¤ºè¯¦ç»†æŠ¥å‘Š
        await display_quality_reports(reports)
        
        # ç”Ÿæˆç»Ÿè®¡åˆ†æ
        await generate_quality_statistics(reports)
        
        # æä¾›æ”¹è¿›å»ºè®®
        await provide_improvement_suggestions(reports)
        
        print_section_header("è´¨é‡ä¿è¯å®Œæˆ", "ğŸ‰")
        print("\nğŸ”— ä¸‹ä¸€æ­¥: è¿è¡Œ 06_advanced_features.py æŸ¥çœ‹é«˜çº§åŠŸèƒ½ç¤ºä¾‹")


if __name__ == "__main__":
    asyncio.run(main())